{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab2: Binary classification and model evaluation\n",
    "---\n",
    "\n",
    "Result of that lab is a **report** in ipynb format. This course is not about coding, so the less code you write the better. It is easier to find mistakes and expand or modify experiments.\n",
    "![E=(mc)^2 : errors = (more code)^2](https://pp.userapi.com/c638722/v638722272/1f4b3/J0mqkFTF0IY.jpg)\n",
    "\n",
    "Try to write you report as interesting story by consequently answer questions from the task. \n",
    "\n",
    "**($\\star$)** questions add no extra points to assessment, but boost your skills and karma a little bit  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Spam classification\n",
    "\n",
    "Let's try to build machine learning models to divide spam from non-spam. \n",
    "\n",
    "Dataset: [UCI](https://archive.ics.uci.edu/ml/datasets.html)\n",
    "\n",
    "Each object in dataset is a letter with features based on text, spam is a positive example, normal letter - negative.\n",
    "\n",
    "#### Task\n",
    "   - Download dataset [Spambase](https://archive.ics.uci.edu/ml/datasets/Spambase) . (Code is provided bellow)\n",
    "   - How many letters in a dataset ?\n",
    "   - Which portion of them is bad (spam) ?\n",
    "   - How you can group letters' features ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Classifier training and  it's evaluation\n",
    "\n",
    "We want our model to generalize well. That means to predict class on the data that we did not see during training phase.\n",
    "In order to achive this model should be trained and evaluated on independent sets of examples. Usually, we divide our dataset to two subsets: `train` and `test`. (sometimes we need to split on 3 sets `train`, `validation`, `test`). How to split the data is a compromise: bigger `train` gives you more information and you can build <font color='red'>_better_</font>  algorithms, on the other hand more control examples from `test` would give you less noisy estimate of model quality. \n",
    "\n",
    "For model evaluation you will analyse [confusion matrix](http://en.wikipedia.org/wiki/Confusion_matrix): Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class. \n",
    "\n",
    "![Confusion matrix](http://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png)\n",
    "\n",
    "Diagonal consists of correctly classified positive (True positive - TP) and negative (TN) examples. There are two types of errors. False Positive (FP) — type I erros (false activation, good letter went to spam), False Negative (FN) — type II errors (did not filter span). <font color='red'>**Errors of type I and II could have different cost.**</font>\n",
    "\n",
    "<img src=\"https://pp.userapi.com/c837427/v837427272/130f4/PJX8E_FvkG8.jpg\" alt=\"Types of errors\" style=\"width: 300px;\"/>\n",
    "\n",
    "For binary classification we have following quality metrics:\n",
    "  - Accuracy = (TP + TN) / (TP + TN + FP + FN)  — _fraction of correct predictions_\n",
    "  - Precision = TP / (TP + FP)  — _accuracy, fraction of real spam in the letters classified as spam_\n",
    "  - Recall = TP / (TP + FN)  — _completeness, fraction of filtered spam_\n",
    "  - F1 = 2TP / (2TP + FP + FN)  — _harmonic mean of precision and recall_\n",
    "  \n",
    "More information you can find here: [Precision and Recall](http://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "#### Task \n",
    "   - Split dataset into to disjoint subsets: `train` - first 3000 examples (≈65%), `test` - all others\n",
    "   - Train decesion tree with `train`. Classify examples from `test`. Calculate classification quality metrics, described above: Accuracy, Precision, Recall, F1.  [Recommended parameters: split criteria - gini, max_depth: 7]\n",
    "   - Which drawbacks does evaluation on this `test` have? How you can make evalution more informative?\n",
    "   - Repeat experiment with dataset shuffled before `split`.\n",
    "   - Which features are the most informative? Use feature importance from DecisionTreeClassifier.\n",
    "   - What you can say about quality of the best constant model (constant model always predicts one class)\n",
    "   - Train KNN model and evaluate it on `test`. [ Recommended parameters: K=10, euclidian metric ]\n",
    "   - Train KNN for rescaled features. Evaluate quality of the model on `test`. Does that feature transformation increased given metrics for KNN? Repeat experiment for DecisionTree. Why rescaling has no effect on the quality of decision tree?\n",
    "   - Compare all models by all metrics.\n",
    "   - ($\\star$) To find better `train_test_split` you can conduct following experiment: grid search split proportion, for every threshold generate big number of splittings and compute error (according to chosen metric). Then plot dependence of error on split threshold. Choose threshold that gives least variation of the error. Choose metric and plot error standard deviation as function of split threshold. \n",
    "   \n",
    "  Rescaling methods:\n",
    "  - $x_{new} = \\frac{x - \\mu}{\\sigma}$, $\\mu, \\sigma$ — mean, and standard deviation\n",
    "  - $x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}}$, $[x_{min}, x_{max}]$ — minimal interval of features' values\n",
    "  \n",
    "  As a result of this task you should get following models and compare them between each other: \n",
    "  1. Constant model\n",
    "  2. Decision tree\n",
    "  3. KNN\n",
    "  4. KNN with rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Classification threshold selection\n",
    "\n",
    "Many classification models give estimation of  belonging to the class  $\\tilde{y}(x) \\in R$. Then decision about the class is made by comparing that estimation to some threshold:\n",
    "\n",
    "$y(x) = +1$,  if $\\tilde{y}(x) \\geq \\theta$, $y(x) = -1$, if $\\tilde{y}(x) < \\theta$\n",
    "\n",
    "Confusion matrix and all derivatives (Accuracy, Precision, Recall, etc.) depend on $\\theta$:\n",
    "\n",
    "In order to show the variation of metrics from threshold, you can draw coordinate plane with axis as metrics and the shape of the figure will show you quality of the model.  \n",
    "![ROC-curve construction principle and threshold](https://upload.wikimedia.org/wikipedia/commons/8/8c/Receiver_Operating_Characteristic.png)\n",
    "\n",
    "Most popular curve from such class is ROC-curve (TP-vs-FP plane) and Precision/Recall curves. ROC-curve stands for [Receiver Operating Characteristic](en.wikipedia.org/wiki/Receiver_operating_characteristic).\n",
    "![ROC-curves examples](http://arogozhnikov.github.io/images/roc_curve.gif)\n",
    "\n",
    "[interactive example](http://arogozhnikov.github.io/2015/10/05/roc-curve.html)\n",
    "\n",
    "In the case, if you need to compare quality of classifier without assumptions about threshold, you can use summary statistics or integral metrics based on ROC curve. For example AUC-ROC (**A**rea **U**nder RO**C**) - area under the ROC curve of classifier. AUC-ROC of ideal classifier is 1. Ideal **random** classifier has AUC-ROC about 0.5.\n",
    "\n",
    "\n",
    "#### Task\n",
    "  - Whick classifier has AUC-ROC near 0?\n",
    "  - Is it correct, that classifier from previous part #2 to identify class compare some estimatino with threshold? What are those estimations? What thresholds were chosen?\n",
    "  - For all models from previous task:\n",
    "      - Draw ROC and Precision/Recall curves on the same coordinate plane with different colors. Add legend: which curve corresponds to which classifier.\n",
    "      - Compare AUC-ROC.\n",
    "  - What maximal Recall of spam classification can models achive if you have strict constraint on accuracy: it should not be below 90%?\n",
    "  - ($\\star$) Mark classifiers builded in the previous part with points of different color and shape on the Precision/Recall plane. Add legend: which points correspond to which model. Show isoline of F1 metric (where F1 metric has the same value).\n",
    "  - ($\\star$) Suggest exact effective algorithm to compute AUC-ROC metric with complexity $O(n \\log n)$, where $n$ - number of test examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Cross-validation and parameter selection\n",
    "\n",
    "Each model of machine learning has many structural parameters and parameters of learning: number of neighbours, length function in KNN, max tree depth, mininal number of objects in leafs and so on. There is no universal set of parameters which would be optimal for all given tasks. For every new task you should select other set of parameters.\n",
    "\n",
    "For model's parameters' optimization researchers usually use _grid search_ : pick several values for every parameter, then evaluate every combination of parameters, and choose the best set from the point of optimized metric.\n",
    "\n",
    "If you try a lot of models, it might appear that the best model on `train` did not perserve it's quality on `test` set. We can conclude that in this case model was _overfitted_ to given `train`. \n",
    "\n",
    "To get rid of that problem we can split out dataset on 3 disjoint subsets: `train`, `validation` and `test`\n",
    "![Разбиение на train, validation и test](http://2.bp.blogspot.com/-jkEGMO5lb8A/VmIj1SWT8KI/AAAAAAAAAAs/XTUClFffcX4/s1600/Screen%2BShot%2B2015-12-04%2Bat%2B6.37.34%2BPM.png)\n",
    "\n",
    "`Validation` set is used to compare models when tuning hyperparameters of the model. `Test` is for final assessment of the models.\n",
    "\n",
    "There is more robust method of objective model comparison - [cross-validation](http://en.wikipedia.org/wiki/Cross-validation_(statistics)). \n",
    "There exist different types of that general scheme: \n",
    "   - Leave-One-Out\n",
    "   - K-Fold\n",
    "   - Repeated random sub-sampling\n",
    "\n",
    "Cross-validation is computationally expensive, especially if you do exhaustive grid search with big number of combinations. You should consider following trade-offs:\n",
    "   - you can take more sparse grid, considering less values for each parameter. But you can miss good combination then;\n",
    "   - you can use less partitions or folds, in that case the estimate of cross-validation score becomes more noisy and you have higher risk to make not optimal set of parameters, because of the randomness of partition;\n",
    "   - you can optimize parameters greedily (one by one, consequently), that strategy is not always optimal;\n",
    "   - use random sub-sampling of parameters instead of exhaustive search.\n",
    "   \n",
    "#### Task\n",
    "\n",
    "   - Choose cross-validation method, one from described above. Fixate cross-validation split of `train` set. You should cross-validate on `train` samples from previous tasks, `test` should stay independent. Pay attention: when comparing models, cross-validation split must not change.\n",
    "   - Choose one metric for optimization by grid search. _Example: AUC-ROC._\n",
    "   - Find optimal set of parameters for tree with grid search. Parameters for grid: split criterion, max depth, number of features for node, min number of objects in the leaf (of only some of offered parameters)\n",
    "   - Find optimal set of parameters for KNN using grid search. Parameters for grid: K, metric, weight scheme.\n",
    "   - ($\\star$) [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) -  any test or metric that relies on random sampling with replacement. Can you use different bootstrapping methods for training and evaluating your models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Usefull functions of SciKit-Learn\n",
    "\n",
    "These functions will make compliting this lab a pleasure for you.\n",
    "\n",
    "- Module for quality evaluation and cross-validation [`sklearn.cross_validation`](http://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "    - dataset split [`train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split)\n",
    "    - iterators for cross-validation splits [`LeaveOneOut`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.LeaveOneOut.html#sklearn.cross_validation.LeaveOneOut), [`KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold), [`ShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.ShuffleSplit.html#sklearn.cross_validation.ShuffleSplit)\n",
    "    - always use `random_state` parameter, it will make your experiments reproducible.\n",
    "- Module with metrics [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "  - [`accuracy_score(y_true, y_pred)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "  - [`precision_recall_fscore_support(y_true, y_pred)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)\n",
    "  - [`roc_curve(y_true, y_score)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)\n",
    "  - [`precision_recall_curve(y_true, y_score)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)\n",
    "  - [`roc_auc_score(y_true, y_score)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)\n",
    "- Module for data preprocessing [`sklearn.preprocessing`](http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "    - function for feature standartization [`scale(X)`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale)\n",
    "    - class that helps to map your features into [0-1] interval [`MinMaxScaler().fit_transform(X)`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    "- Module for grid search [`sklearn.grid_search`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.grid_search)\n",
    "    - Class [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV) - implements exhaustive search on a given grid\n",
    "    - Class [`RandomizedSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn.grid_search.RandomizedSearchCV) - implements random search of parameters\n",
    "- Class for constant classifier  [`DummyClassifier(strategy='constant', constant=0)`](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier)\n",
    "\n",
    "  \n",
    "### Manual, docs and examples\n",
    "\n",
    "- [Model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Parameter optimization with cross-val grid search](http://scikit-learn.org/stable/auto_examples/grid_search_digits.html)\n",
    "- [ROC-curves](http://scikit-learn.org/stable/auto_examples/plot_roc.html), [Precision-Recall кривые](http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html)\n",
    "- [Feature importance in decesion trees](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use this jupyter notebook **`magic`** to be able to plot figures directly in notebook.\n",
    "Docs: [`Maplotlib`](http://matplotlib.org/) and [`pylab`](http://wiki.scipy.org/PyLab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spambase dataset downloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...   char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...          0.00        0.000   \n",
       "1             0.00            0.94  ...          0.00        0.132   \n",
       "2             0.64            0.25  ...          0.01        0.143   \n",
       "3             0.31            0.63  ...          0.00        0.137   \n",
       "4             0.31            0.63  ...          0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import urllib2\n",
    "\n",
    "SPAMBASE_NAMES_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names'\n",
    "SPAMBASE_DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "\n",
    "feature_names = [\n",
    "    line.strip().split(':')[0] \n",
    "    for line in urllib2.urlopen(SPAMBASE_NAMES_URL).readlines()[33:]\n",
    "]\n",
    "spam_data = pandas.read_csv(SPAMBASE_DATA_URL, header=None, names=(feature_names + ['spam']))\n",
    " \n",
    "X, y = spam_data.ix[:, :-1].values, spam_data.ix[:, -1].values\n",
    " \n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DecisionTree training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris_data = load_iris()\n",
    "X, y = iris_data.data, iris_data.target\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3).fit(X, y)\n",
    "\n",
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Feature importances are calculated based on the frequency of the split by the feature and the depth of the nodes of the tree in which the split took place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 petal width (cm), importance = 0.95\n",
      "2 petal length (cm), importance = 0.05\n",
      "1 sepal width (cm), importance = 0.00\n",
      "0 sepal length (cm), importance = 0.00\n"
     ]
    }
   ],
   "source": [
    "most_important_features = argsort(clf.feature_importances_)[::-1]\n",
    "for idx in most_important_features:\n",
    "    print '%d %s, importance = %.2f' % (idx, iris_data.feature_names[idx], clf.feature_importances_[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Misclassified objects of `train` set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xc36bcf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc2e3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xind, yind = most_important_features[0], most_important_features[1]\n",
    "err = (y != y_pred)\n",
    "scatter(X[err, xind], X[err, yind], c=y_pred[err], marker='x', s=80, linewidths=2)\n",
    "scatter(X[:, xind], X[:, yind], c=y)\n",
    "xlabel(iris_data.feature_names[xind])\n",
    "ylabel(iris_data.feature_names[yind])\n",
    "title(u'Classification errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Tree visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(clf, out_file='tree.dot', feature_names=iris_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to use bash type !command, it might require **conda install graphviz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"dot\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!dot -Tpng tree.dot -o tree.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "dHJlZS5wbmc=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_data = load_boston()\n",
    "X, y = boston_data.data, boston_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fix 5-Fold partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "cv = KFold(n=len(y), n_folds=5, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pay attention for the arguments:\n",
    "  - `shuffle=True` - it shuffles dataset before partitioning, without it every partition would consist of consequent elements which is bad (you can see that in task #2)\n",
    "  - `random_state` - fixate random state, makes your partition reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Evaluate quality of KNN regression of the Boston Housing dataset for different values of K (number of neighbours). Consider mean absolute error as a quality metric:\n",
    "$$MAE = \\sum_i|y_{pred,i} - y_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's try different `n_neighbors` values, for each value evaluate mean and stdandard deviation of MAE using cross-validation. [Magic `%%time`](http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb) will help you measure time of code execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 352 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "k_values = range(1, 10) + range(10, 50, 5)\n",
    "mae_cv_mean = []\n",
    "mae_cv_std = []\n",
    "\n",
    "for k in k_values:\n",
    "    clf = KNeighborsRegressor(n_neighbors=k)\n",
    "    mae_folds = -cross_val_score(clf, X, y, cv=cv, scoring='mean_absolute_error')\n",
    "    mae_cv_mean.append(mae_folds.mean())\n",
    "    mae_cv_std.append(mae_folds.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Figure, that describes optimality of parameter selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Annotation at 0xc7a4358>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEXCAYAAAC0xN2YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc78cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(figsize=(8, 4))\n",
    "errorbar(k_values, mae_cv_mean, yerr=array(mae_cv_std)*2)\n",
    "title('Choosing optimal number of neighbors with 5-Fold Cross-Validation')\n",
    "xlabel('n_neighbors')\n",
    "ylabel('mean absolute error')\n",
    "\n",
    "opt_idx = argmin(mae_cv_mean)\n",
    "optimal_k = k_values[opt_idx]\n",
    "optimal_mae = mae_cv_mean[opt_idx]\n",
    "annotate('n_neighbors=%d\\nMAE=%f' % (optimal_k, optimal_mae), \n",
    "         xy=(opt_idx, optimal_mae), xytext=(30, optimal_mae), \n",
    "         arrowprops=dict(facecolor='black', shrink=0.05, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Spam classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.671</td>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.450</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.022</td>\n",
       "      <td>9.744</td>\n",
       "      <td>445</td>\n",
       "      <td>1257</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.729</td>\n",
       "      <td>43</td>\n",
       "      <td>749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.312</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.243</td>\n",
       "      <td>11</td>\n",
       "      <td>184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.728</td>\n",
       "      <td>61</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.083</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.971</td>\n",
       "      <td>24</td>\n",
       "      <td>205</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.659</td>\n",
       "      <td>55</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.652</td>\n",
       "      <td>31</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.975</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.461</td>\n",
       "      <td>95</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.320</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.509</td>\n",
       "      <td>91</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.833</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.059</td>\n",
       "      <td>2.569</td>\n",
       "      <td>66</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.857</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.131</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.466</td>\n",
       "      <td>22</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.059</td>\n",
       "      <td>2.565</td>\n",
       "      <td>66</td>\n",
       "      <td>2258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.466</td>\n",
       "      <td>22</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.611</td>\n",
       "      <td>12</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.687</td>\n",
       "      <td>66</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.256</td>\n",
       "      <td>5</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.489</td>\n",
       "      <td>11</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.220</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.720</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.488</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.200</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.372</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.202</td>\n",
       "      <td>3.766</td>\n",
       "      <td>43</td>\n",
       "      <td>1789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4580</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.571</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4581</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.586</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.266</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4583</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.666</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>7</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4585</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.375</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>1.793</td>\n",
       "      <td>21</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.272</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.111</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.092</td>\n",
       "      <td>2.468</td>\n",
       "      <td>11</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4591</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.285</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4593</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.727</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0               0.00               0.64           0.64           0.0   \n",
       "1               0.21               0.28           0.50           0.0   \n",
       "2               0.06               0.00           0.71           0.0   \n",
       "3               0.00               0.00           0.00           0.0   \n",
       "4               0.00               0.00           0.00           0.0   \n",
       "5               0.00               0.00           0.00           0.0   \n",
       "6               0.00               0.00           0.00           0.0   \n",
       "7               0.00               0.00           0.00           0.0   \n",
       "8               0.15               0.00           0.46           0.0   \n",
       "9               0.06               0.12           0.77           0.0   \n",
       "10              0.00               0.00           0.00           0.0   \n",
       "11              0.00               0.00           0.25           0.0   \n",
       "12              0.00               0.69           0.34           0.0   \n",
       "13              0.00               0.00           0.00           0.0   \n",
       "14              0.00               0.00           1.42           0.0   \n",
       "15              0.00               0.42           0.42           0.0   \n",
       "16              0.00               0.00           0.00           0.0   \n",
       "17              0.00               0.00           0.00           0.0   \n",
       "18              0.00               0.00           0.55           0.0   \n",
       "19              0.00               0.63           0.00           0.0   \n",
       "20              0.00               0.00           0.00           0.0   \n",
       "21              0.05               0.07           0.10           0.0   \n",
       "22              0.00               0.00           0.00           0.0   \n",
       "23              0.00               0.00           0.00           0.0   \n",
       "24              0.00               0.00           0.00           0.0   \n",
       "25              0.05               0.07           0.10           0.0   \n",
       "26              0.00               0.00           0.00           0.0   \n",
       "27              0.00               0.00           0.00           0.0   \n",
       "28              0.00               0.00           0.00           0.0   \n",
       "29              0.00               0.00           0.00           0.0   \n",
       "...              ...                ...            ...           ...   \n",
       "4571            0.00               0.00           0.46           0.0   \n",
       "4572            0.00               0.00           0.00           0.0   \n",
       "4573            0.00               0.00           0.18           0.0   \n",
       "4574            0.29               0.00           0.29           0.0   \n",
       "4575            0.00               0.00           0.00           0.0   \n",
       "4576            0.00               0.00           0.00           0.0   \n",
       "4577            0.00               0.00           1.20           0.0   \n",
       "4578            0.00               0.00           0.40           0.0   \n",
       "4579            0.27               0.05           0.10           0.0   \n",
       "4580            0.00               0.00           0.00           0.0   \n",
       "4581            0.00               0.00           0.00           0.0   \n",
       "4582            0.00               0.00           0.00           0.0   \n",
       "4583            0.00               0.00           1.23           0.0   \n",
       "4584            0.00               0.00           0.45           0.0   \n",
       "4585            0.00               0.00           0.00           0.0   \n",
       "4586            0.00               0.00           0.00           0.0   \n",
       "4587            0.00               0.00           0.00           0.0   \n",
       "4588            0.00               0.00           3.03           0.0   \n",
       "4589            0.00               0.00           0.00           0.0   \n",
       "4590            0.00               0.00           0.00           0.0   \n",
       "4591            0.00               0.00           0.00           0.0   \n",
       "4592            0.00               0.00           1.25           0.0   \n",
       "4593            0.00               0.00           0.00           0.0   \n",
       "4594            0.00               0.00           0.00           0.0   \n",
       "4595            0.00               0.00           1.19           0.0   \n",
       "4596            0.31               0.00           0.62           0.0   \n",
       "4597            0.00               0.00           0.00           0.0   \n",
       "4598            0.30               0.00           0.30           0.0   \n",
       "4599            0.96               0.00           0.00           0.0   \n",
       "4600            0.00               0.00           0.65           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0              0.32            0.00              0.00                0.00   \n",
       "1              0.14            0.28              0.21                0.07   \n",
       "2              1.23            0.19              0.19                0.12   \n",
       "3              0.63            0.00              0.31                0.63   \n",
       "4              0.63            0.00              0.31                0.63   \n",
       "5              1.85            0.00              0.00                1.85   \n",
       "6              1.92            0.00              0.00                0.00   \n",
       "7              1.88            0.00              0.00                1.88   \n",
       "8              0.61            0.00              0.30                0.00   \n",
       "9              0.19            0.32              0.38                0.00   \n",
       "10             0.00            0.00              0.96                0.00   \n",
       "11             0.38            0.25              0.25                0.00   \n",
       "12             0.34            0.00              0.00                0.00   \n",
       "13             0.90            0.00              0.90                0.00   \n",
       "14             0.71            0.35              0.00                0.35   \n",
       "15             1.27            0.00              0.42                0.00   \n",
       "16             0.94            0.00              0.00                0.00   \n",
       "17             0.00            0.00              0.00                0.00   \n",
       "18             1.11            0.00              0.18                0.00   \n",
       "19             1.59            0.31              0.00                0.00   \n",
       "20             0.00            0.00              0.00                0.00   \n",
       "21             0.76            0.05              0.15                0.02   \n",
       "22             2.94            0.00              0.00                0.00   \n",
       "23             1.16            0.00              0.00                0.00   \n",
       "24             0.00            0.00              0.00                0.00   \n",
       "25             0.76            0.05              0.15                0.02   \n",
       "26             0.00            0.00              0.00                0.00   \n",
       "27             0.00            0.00              1.66                0.00   \n",
       "28             0.00            0.00              0.00                0.00   \n",
       "29             0.65            0.00              0.65                0.00   \n",
       "...             ...             ...               ...                 ...   \n",
       "4571           0.23            0.23              0.00                0.00   \n",
       "4572           0.00            0.00              0.00                0.00   \n",
       "4573           0.18            0.18              0.00                0.00   \n",
       "4574           0.00            0.00              0.00                0.00   \n",
       "4575           0.00            0.00              0.00                0.00   \n",
       "4576           0.00            0.00              0.00                0.00   \n",
       "4577           0.00            0.00              0.00                0.00   \n",
       "4578           0.00            0.00              0.00                0.00   \n",
       "4579           0.00            0.00              0.00                0.00   \n",
       "4580           0.00            0.00              0.00                0.00   \n",
       "4581           0.00            0.51              0.00                0.00   \n",
       "4582           0.00            0.00              0.00                0.00   \n",
       "4583           0.00            0.00              0.00                0.00   \n",
       "4584           0.00            0.22              0.00                0.00   \n",
       "4585           0.00            0.00              0.00                0.00   \n",
       "4586           0.36            0.00              0.00                0.00   \n",
       "4587           0.00            0.00              0.00                0.00   \n",
       "4588           0.00            0.00              0.00                0.00   \n",
       "4589           0.54            0.00              0.00                0.00   \n",
       "4590           0.00            0.00              0.00                0.00   \n",
       "4591           0.00            0.00              0.00                0.00   \n",
       "4592           2.50            0.00              0.00                0.00   \n",
       "4593           0.00            0.00              0.00                0.00   \n",
       "4594           0.00            0.00              0.00                0.00   \n",
       "4595           0.00            0.00              0.00                0.00   \n",
       "4596           0.00            0.31              0.00                0.00   \n",
       "4597           0.00            0.00              0.00                0.00   \n",
       "4598           0.00            0.00              0.00                0.00   \n",
       "4599           0.32            0.00              0.00                0.00   \n",
       "4600           0.00            0.00              0.00                0.00   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...   char_freq_;  char_freq_(  \\\n",
       "0                0.00            0.00  ...         0.000        0.000   \n",
       "1                0.00            0.94  ...         0.000        0.132   \n",
       "2                0.64            0.25  ...         0.010        0.143   \n",
       "3                0.31            0.63  ...         0.000        0.137   \n",
       "4                0.31            0.63  ...         0.000        0.135   \n",
       "5                0.00            0.00  ...         0.000        0.223   \n",
       "6                0.00            0.64  ...         0.000        0.054   \n",
       "7                0.00            0.00  ...         0.000        0.206   \n",
       "8                0.92            0.76  ...         0.000        0.271   \n",
       "9                0.06            0.00  ...         0.040        0.030   \n",
       "10               0.00            1.92  ...         0.000        0.000   \n",
       "11               0.00            0.00  ...         0.022        0.044   \n",
       "12               0.00            0.00  ...         0.000        0.056   \n",
       "13               0.00            0.90  ...         0.000        0.000   \n",
       "14               0.00            0.71  ...         0.000        0.102   \n",
       "15               0.00            1.27  ...         0.000        0.063   \n",
       "16               0.00            0.00  ...         0.000        0.000   \n",
       "17               0.00            0.00  ...         0.000        0.000   \n",
       "18               0.00            0.00  ...         0.000        0.182   \n",
       "19               0.31            0.00  ...         0.000        0.275   \n",
       "20               0.00            0.00  ...         0.000        0.729   \n",
       "21               0.55            0.00  ...         0.042        0.101   \n",
       "22               0.00            0.00  ...         0.404        0.404   \n",
       "23               0.00            0.00  ...         0.000        0.133   \n",
       "24               0.00            0.00  ...         0.000        0.196   \n",
       "25               0.55            0.00  ...         0.042        0.101   \n",
       "26               0.00            0.00  ...         0.000        0.196   \n",
       "27               0.00            0.00  ...         0.000        0.000   \n",
       "28               0.00            0.00  ...         0.000        0.352   \n",
       "29               0.00            0.00  ...         0.000        0.459   \n",
       "...               ...             ...  ...           ...          ...   \n",
       "4571             0.00            0.00  ...         0.000        0.082   \n",
       "4572             0.00            0.00  ...         0.000        0.254   \n",
       "4573             0.00            0.00  ...         0.033        0.033   \n",
       "4574             0.00            0.29  ...         0.000        0.107   \n",
       "4575             0.00            1.38  ...         0.000        0.213   \n",
       "4576             0.00            0.00  ...         0.000        0.131   \n",
       "4577             0.00            0.00  ...         0.000        0.000   \n",
       "4578             0.00            0.00  ...         0.000        0.000   \n",
       "4579             0.00            0.00  ...         0.607        0.064   \n",
       "4580             0.00            0.00  ...         0.000        0.000   \n",
       "4581             0.00            0.00  ...         0.000        0.091   \n",
       "4582             0.00            0.00  ...         0.000        0.000   \n",
       "4583             0.00            0.00  ...         0.000        0.000   \n",
       "4584             0.00            0.00  ...         0.000        0.082   \n",
       "4585             0.00            0.00  ...         0.000        0.625   \n",
       "4586             0.00            0.00  ...         0.000        0.112   \n",
       "4587             0.00            0.00  ...         0.000        0.125   \n",
       "4588             0.00            0.00  ...         0.000        0.000   \n",
       "4589             0.00            0.00  ...         0.000        0.000   \n",
       "4590             0.00            0.00  ...         0.000        0.185   \n",
       "4591             0.00            0.00  ...         0.000        0.000   \n",
       "4592             0.00            0.00  ...         0.000        0.111   \n",
       "4593             0.00            0.00  ...         0.000        0.000   \n",
       "4594             0.00            0.00  ...         0.000        0.630   \n",
       "4595             0.00            0.00  ...         0.000        0.000   \n",
       "4596             0.00            0.00  ...         0.000        0.232   \n",
       "4597             0.00            0.00  ...         0.000        0.000   \n",
       "4598             0.00            0.00  ...         0.102        0.718   \n",
       "4599             0.00            0.00  ...         0.000        0.057   \n",
       "4600             0.00            0.00  ...         0.000        0.000   \n",
       "\n",
       "      char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0           0.000        0.778        0.000        0.000   \n",
       "1           0.000        0.372        0.180        0.048   \n",
       "2           0.000        0.276        0.184        0.010   \n",
       "3           0.000        0.137        0.000        0.000   \n",
       "4           0.000        0.135        0.000        0.000   \n",
       "5           0.000        0.000        0.000        0.000   \n",
       "6           0.000        0.164        0.054        0.000   \n",
       "7           0.000        0.000        0.000        0.000   \n",
       "8           0.000        0.181        0.203        0.022   \n",
       "9           0.000        0.244        0.081        0.000   \n",
       "10          0.000        0.462        0.000        0.000   \n",
       "11          0.000        0.663        0.000        0.000   \n",
       "12          0.000        0.786        0.000        0.000   \n",
       "13          0.000        0.000        0.000        0.000   \n",
       "14          0.000        0.357        0.000        0.000   \n",
       "15          0.000        0.572        0.063        0.000   \n",
       "16          0.000        0.428        0.000        0.000   \n",
       "17          0.000        1.975        0.370        0.000   \n",
       "18          0.000        0.455        0.000        0.000   \n",
       "19          0.000        0.055        0.496        0.000   \n",
       "20          0.000        0.729        0.000        0.000   \n",
       "21          0.016        0.250        0.046        0.059   \n",
       "22          0.000        0.809        0.000        0.000   \n",
       "23          0.000        0.667        0.000        0.000   \n",
       "24          0.000        0.392        0.196        0.000   \n",
       "25          0.016        0.250        0.046        0.059   \n",
       "26          0.000        0.392        0.196        0.000   \n",
       "27          0.000        0.368        0.000        0.000   \n",
       "28          0.000        0.352        0.000        0.000   \n",
       "29          0.000        0.091        0.000        0.000   \n",
       "...           ...          ...          ...          ...   \n",
       "4571        0.000        0.082        0.000        0.000   \n",
       "4572        0.000        0.000        0.000        0.000   \n",
       "4573        0.000        0.099        0.000        0.000   \n",
       "4574        0.000        0.000        0.000        0.000   \n",
       "4575        0.000        0.000        0.000        0.000   \n",
       "4576        0.000        0.000        0.000        0.000   \n",
       "4577        0.000        0.000        0.000        0.000   \n",
       "4578        0.145        0.000        0.000        0.000   \n",
       "4579        0.036        0.055        0.000        0.202   \n",
       "4580        0.000        0.000        0.000        0.000   \n",
       "4581        0.000        0.091        0.000        0.000   \n",
       "4582        0.000        0.000        0.000        0.000   \n",
       "4583        0.406        0.000        0.000        0.000   \n",
       "4584        0.000        0.041        0.000        0.000   \n",
       "4585        0.000        0.000        0.000        0.000   \n",
       "4586        0.000        0.000        0.000        0.056   \n",
       "4587        0.000        0.000        0.125        0.000   \n",
       "4588        0.000        0.000        0.000        0.000   \n",
       "4589        0.000        0.000        0.000        0.000   \n",
       "4590        0.000        0.000        0.000        0.092   \n",
       "4591        0.000        0.000        0.000        0.000   \n",
       "4592        0.000        0.000        0.000        0.000   \n",
       "4593        0.000        1.052        0.000        0.000   \n",
       "4594        0.000        0.000        0.000        0.000   \n",
       "4595        0.000        0.000        0.000        0.000   \n",
       "4596        0.000        0.000        0.000        0.000   \n",
       "4597        0.000        0.353        0.000        0.000   \n",
       "4598        0.000        0.000        0.000        0.000   \n",
       "4599        0.000        0.000        0.000        0.000   \n",
       "4600        0.000        0.125        0.000        0.000   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "0                          3.756                          61   \n",
       "1                          5.114                         101   \n",
       "2                          9.821                         485   \n",
       "3                          3.537                          40   \n",
       "4                          3.537                          40   \n",
       "5                          3.000                          15   \n",
       "6                          1.671                           4   \n",
       "7                          2.450                          11   \n",
       "8                          9.744                         445   \n",
       "9                          1.729                          43   \n",
       "10                         1.312                           6   \n",
       "11                         1.243                          11   \n",
       "12                         3.728                          61   \n",
       "13                         2.083                           7   \n",
       "14                         1.971                          24   \n",
       "15                         5.659                          55   \n",
       "16                         4.652                          31   \n",
       "17                        35.461                          95   \n",
       "18                         1.320                           4   \n",
       "19                         3.509                          91   \n",
       "20                         3.833                           9   \n",
       "21                         2.569                          66   \n",
       "22                         4.857                          12   \n",
       "23                         1.131                           5   \n",
       "24                         5.466                          22   \n",
       "25                         2.565                          66   \n",
       "26                         5.466                          22   \n",
       "27                         2.611                          12   \n",
       "28                         4.000                          11   \n",
       "29                         2.687                          66   \n",
       "...                          ...                         ...   \n",
       "4571                       1.256                           5   \n",
       "4572                       1.000                           1   \n",
       "4573                       1.489                          11   \n",
       "4574                       1.220                           6   \n",
       "4575                       1.720                          11   \n",
       "4576                       1.488                           5   \n",
       "4577                       1.200                           3   \n",
       "4578                       1.372                           5   \n",
       "4579                       3.766                          43   \n",
       "4580                       1.571                           5   \n",
       "4581                       1.586                           4   \n",
       "4582                       1.266                           3   \n",
       "4583                       1.666                          13   \n",
       "4584                       1.500                           7   \n",
       "4585                       1.375                           4   \n",
       "4586                       1.793                          21   \n",
       "4587                       1.272                           4   \n",
       "4588                       1.111                           2   \n",
       "4589                       1.000                           1   \n",
       "4590                       2.468                          11   \n",
       "4591                       1.000                           1   \n",
       "4592                       1.285                           4   \n",
       "4593                       1.000                           1   \n",
       "4594                       1.727                           5   \n",
       "4595                       1.000                           1   \n",
       "4596                       1.142                           3   \n",
       "4597                       1.555                           4   \n",
       "4598                       1.404                           6   \n",
       "4599                       1.147                           5   \n",
       "4600                       1.250                           5   \n",
       "\n",
       "      capital_run_length_total  spam  \n",
       "0                          278     1  \n",
       "1                         1028     1  \n",
       "2                         2259     1  \n",
       "3                          191     1  \n",
       "4                          191     1  \n",
       "5                           54     1  \n",
       "6                          112     1  \n",
       "7                           49     1  \n",
       "8                         1257     1  \n",
       "9                          749     1  \n",
       "10                          21     1  \n",
       "11                         184     1  \n",
       "12                         261     1  \n",
       "13                          25     1  \n",
       "14                         205     1  \n",
       "15                         249     1  \n",
       "16                         107     1  \n",
       "17                         461     1  \n",
       "18                          70     1  \n",
       "19                         186     1  \n",
       "20                          23     1  \n",
       "21                        2259     1  \n",
       "22                          34     1  \n",
       "23                          69     1  \n",
       "24                          82     1  \n",
       "25                        2258     1  \n",
       "26                          82     1  \n",
       "27                          47     1  \n",
       "28                          36     1  \n",
       "29                         129     1  \n",
       "...                        ...   ...  \n",
       "4571                        98     0  \n",
       "4572                        13     0  \n",
       "4573                       137     0  \n",
       "4574                        61     0  \n",
       "4575                        43     0  \n",
       "4576                        64     0  \n",
       "4577                        24     0  \n",
       "4578                        70     0  \n",
       "4579                      1789     0  \n",
       "4580                        11     0  \n",
       "4581                        46     0  \n",
       "4582                        19     0  \n",
       "4583                        70     0  \n",
       "4584                       123     0  \n",
       "4585                        11     0  \n",
       "4586                       174     0  \n",
       "4587                        28     0  \n",
       "4588                        10     0  \n",
       "4589                        22     0  \n",
       "4590                        79     0  \n",
       "4591                         8     0  \n",
       "4592                        27     0  \n",
       "4593                         6     0  \n",
       "4594                        19     0  \n",
       "4595                        24     0  \n",
       "4596                        88     0  \n",
       "4597                        14     0  \n",
       "4598                       118     0  \n",
       "4599                        78     0  \n",
       "4600                        40     0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spambase_names = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names'\n",
    "spambase_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "\n",
    "feature_names = [\n",
    "    line.strip().split(':')[0] \n",
    "    for line in urllib2.urlopen(spambase_names).readlines()[33:]\n",
    "]\n",
    "spam_data = pd.read_csv(spambase_data, header=None, names=(feature_names + ['spam']))\n",
    " \n",
    "X, y = spam_data.ix[:, :-1].values, spam_data.ix[:, -1].values\n",
    " \n",
    "spam_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many letters in a dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letters count = 4601\n"
     ]
    }
   ],
   "source": [
    "print \"Letters count =\", len(spam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which portion of them is bad (spam)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam percent = 39.4044772875 %\n"
     ]
    }
   ],
   "source": [
    "print \"Spam percent =\", len(spam_data.groupby('spam').get_group(1)) / float(len(spam_data)) * 100, '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How you can group letters' features ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first group describes percentage of * char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1st group = 6\n"
     ]
    }
   ],
   "source": [
    "print 'The 1st group =', len([x for x in spam_data.columns.values if x.startswith('char_freq_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second group describes percentage of * word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2nd group = 48\n"
     ]
    }
   ],
   "source": [
    "print 'The 2nd group =', len([x for x in spam_data.columns.values if x.startswith('word_freq_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third group describes a property of uninterrupted sequences of capital letters\n",
    "capital_run_length_average = average length of uninterrupted sequences of capital letters\n",
    "capital_run_length_longest = length of longest uninterrupted sequence of capital letters\n",
    "capital_run_length_total = total number of capital letters in the e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3rd group = ['capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total']\n"
     ]
    }
   ],
   "source": [
    "print 'The 3rd group =', ([x for x in spam_data.columns.values if x.startswith('capital_run_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classifier training and it's evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into to disjoint subsets: train - first 3000 examples (≈65%), test - all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = spam_data[0:3000]\n",
    "test = spam_data[3000:]\n",
    "\n",
    "train, train_res = train.drop('spam', 1), list(train['spam'])\n",
    "test, test_res = test.drop('spam', 1), list(test['spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train decesion tree with train. Classify examples from test. Calculate classification quality metrics, described above: Accuracy, Precision, Recall, F1. [Recommended parameters: split criteria - gini, max_depth: 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X, y = train, train_res\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=7, random_state=random_state).fit(X, y)\n",
    "\n",
    "dt = {}\n",
    "dt['pred'] = clf.predict(test)\n",
    "dt['pred_prob'] = [x[1] for x in clf.predict_proba(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "\n",
    "def get_confus_matrix(model, y_true):\n",
    "    model['acc'] = accuracy_score(y_true, model['pred'])\n",
    "    model['prec'] = precision_score(y_true, model['pred'])\n",
    "    model['rec'] = recall_score(y_true, model['pred'])\n",
    "    model['f1'] = f1_score(y_true, model['pred'])\n",
    "    \n",
    "def print_confus_matrix(model):\n",
    "    print \"Accuracy =\", model['acc']\n",
    "    print \"Precision =\", model['prec']\n",
    "    print \"Recall =\", model['rec']\n",
    "    print \"F1 =\", model['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.767645221736\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1 = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "get_confus_matrix(dt, test_res)\n",
    "print_confus_matrix(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which drawbacks does evaluation on this test have? How you can make evalution more informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is spam and not spam indices are sequantial? True\n"
     ]
    }
   ],
   "source": [
    "spam_col = list(spam_data['spam'])\n",
    "spam = [i for i in range(len(spam_col)) if spam_col[i] == 1]\n",
    "not_spam = [i for i in range(len(spam_col)) if spam_col[i] == 0]\n",
    "x = range(len(spam)) == spam\n",
    "y = all((np.array(range(len(not_spam))) + len(spam)) == not_spam)\n",
    "print \"Is spam and not spam indices are sequantial?\", x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat experiment with dataset shuffled before split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "rs = ShuffleSplit(test_size=.35, random_state=random_state)\n",
    "\n",
    "for train_index, test_index in rs.split(spam_data):\n",
    "    pass\n",
    "\n",
    "train = spam_data.iloc[train_index]\n",
    "test = spam_data.iloc[test_index]\n",
    "\n",
    "train, train_res = train.drop('spam', 1), list(train['spam'])\n",
    "test, test_res = test.drop('spam', 1), list(test['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = train, train_res\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=7, random_state=random_state).fit(X, y)\n",
    "\n",
    "dts = {}\n",
    "dts['pred'] = clf.predict(test)\n",
    "dts['pred_prob'] = [x[1] for x in clf.predict_proba(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.91495965239\n",
      "Precision = 0.915702479339\n",
      "Recall = 0.865625\n",
      "F1 = 0.889959839357\n"
     ]
    }
   ],
   "source": [
    "get_confus_matrix(dts, test_res)\n",
    "print_confus_matrix(dts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which features are the most informative? Use feature importance from DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 char_freq_!, importance = 0.41\n",
      "54 capital_run_length_average, importance = 0.12\n",
      "6 word_freq_remove, importance = 0.11\n",
      "24 word_freq_hp, importance = 0.07\n",
      "15 word_freq_free, importance = 0.07\n",
      "52 char_freq_$, importance = 0.04\n",
      "16 word_freq_business, importance = 0.02\n",
      "41 word_freq_meeting, importance = 0.02\n",
      "7 word_freq_internet, importance = 0.02\n",
      "45 word_freq_edu, importance = 0.02\n"
     ]
    }
   ],
   "source": [
    "most_important_features = np.argsort(clf.feature_importances_)[::-1]\n",
    "for idx in most_important_features[0:10]:\n",
    "    print '%d %s, importance = %.2f' % (idx, spam_data.columns.values[idx], clf.feature_importances_[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What you can say about quality of the best constant model (constant model always predicts one class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant model with class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "X, y = train, train_res\n",
    "\n",
    "clf = DummyClassifier(strategy='constant', random_state=random_state, constant=0).fit(X, y)\n",
    "\n",
    "const0 = {}\n",
    "const0['pred'] = clf.predict(test)\n",
    "const0['pred_prob'] = [x[1] for x in clf.predict_proba(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.602731222843\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1 = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Elizaveta\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "get_confus_matrix(const0, test_res)\n",
    "print_confus_matrix(const0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = train, train_res\n",
    "\n",
    "clf = DummyClassifier(strategy='constant', random_state=random_state, constant=1).fit(X, y)\n",
    "\n",
    "const1 = {}\n",
    "const1['pred'] = clf.predict(test)\n",
    "const1['pred_prob'] = [x[1] for x in clf.predict_proba(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.397268777157\n",
      "Precision = 0.397268777157\n",
      "Recall = 1.0\n",
      "F1 = 0.568636161706\n"
     ]
    }
   ],
   "source": [
    "get_confus_matrix(const1, test_res)\n",
    "print_confus_matrix(const1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train KNN model and evaluate it on test. [ Recommended parameters: K=10, euclidian metric ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = spam_data.iloc[train_index]\n",
    "test = spam_data.iloc[test_index]\n",
    "\n",
    "train, train_res = train.drop('spam', 1), list(train['spam'])\n",
    "test, test_res = test.drop('spam', 1),  list(test['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.791433891993\n",
      "Precision = 0.781481481481\n",
      "Recall = 0.659375\n",
      "F1 = 0.715254237288\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "\n",
    "X, y = train, train_res\n",
    "neigh.fit(X, y) \n",
    "\n",
    "knn = {}\n",
    "knn['pred'] = neigh.predict(test)\n",
    "knn['pred_prob'] = [x[1] for x in neigh.predict_proba(test)]\n",
    "\n",
    "get_confus_matrix(knn, test_res)\n",
    "print_confus_matrix(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train KNN for rescaled features. Evaluate quality of the model on test. Does that feature transformation increased given metrics for KNN? Repeat experiment for DecisionTree. Why rescaling has no effect on the quality of decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>0.330885</td>\n",
       "      <td>0.712859</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.624007</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.045247</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>-0.008724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345359</td>\n",
       "      <td>0.051909</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.256117</td>\n",
       "      <td>0.672399</td>\n",
       "      <td>0.244743</td>\n",
       "      <td>-0.088010</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>1.086711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.026007</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.126203</td>\n",
       "      <td>0.423783</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>0.250563</td>\n",
       "      <td>1.228324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.145921</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.851723</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>1.364846</td>\n",
       "      <td>0.343685</td>\n",
       "      <td>0.193644</td>\n",
       "      <td>0.036670</td>\n",
       "      <td>1.974017</td>\n",
       "      <td>0.016422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117376</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.440053</td>\n",
       "      <td>-0.079754</td>\n",
       "      <td>0.145921</td>\n",
       "      <td>2.221106</td>\n",
       "      <td>3.258733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.605857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.161934</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.062466</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.605857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.014910</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.164387</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.062466</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>2.286862</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>4.350584</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.310623</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.069076</td>\n",
       "      <td>-0.190757</td>\n",
       "      <td>-0.378189</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>2.390960</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>0.621368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.314548</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.128829</td>\n",
       "      <td>-0.088714</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.110966</td>\n",
       "      <td>-0.247205</td>\n",
       "      <td>-0.282524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>2.331475</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>4.425392</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.247736</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.086412</td>\n",
       "      <td>-0.211283</td>\n",
       "      <td>-0.386436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.148847</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.355778</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.442830</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.474688</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>2.979094</td>\n",
       "      <td>0.807505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.488187</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.107985</td>\n",
       "      <td>0.517334</td>\n",
       "      <td>-0.051802</td>\n",
       "      <td>0.143494</td>\n",
       "      <td>2.015841</td>\n",
       "      <td>1.606036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.145921</td>\n",
       "      <td>-0.072080</td>\n",
       "      <td>0.970750</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.181761</td>\n",
       "      <td>0.818494</td>\n",
       "      <td>0.679083</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.107929</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005855</td>\n",
       "      <td>-0.403329</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.030740</td>\n",
       "      <td>0.021107</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.109138</td>\n",
       "      <td>-0.047071</td>\n",
       "      <td>0.768142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>2.160947</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>2.606833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.236554</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.122282</td>\n",
       "      <td>-0.236941</td>\n",
       "      <td>-0.432619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.060816</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.100792</td>\n",
       "      <td>0.562828</td>\n",
       "      <td>0.346941</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068084</td>\n",
       "      <td>-0.351540</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.483003</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.124457</td>\n",
       "      <td>-0.211283</td>\n",
       "      <td>-0.163768</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>0.369632</td>\n",
       "      <td>0.117725</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.041307</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.307149</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.633816</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.046130</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>-0.036764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.874096</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>2.007651</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>1.024666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.097980</td>\n",
       "      <td>-0.231810</td>\n",
       "      <td>-0.426022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>2.260207</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.591542</td>\n",
       "      <td>0.928066</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>0.610196</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>0.729948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.136984</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.107811</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.101510</td>\n",
       "      <td>-0.144572</td>\n",
       "      <td>-0.129130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.276427</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>1.424331</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.781280</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>1.598589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.281255</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.371427</td>\n",
       "      <td>-0.052107</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>-0.056557</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.933580</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.194866</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.017005</td>\n",
       "      <td>-0.108651</td>\n",
       "      <td>-0.290771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>2.091668</td>\n",
       "      <td>1.196595</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>0.954091</td>\n",
       "      <td>0.219773</td>\n",
       "      <td>0.293116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.534319</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>1.186391</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.168095</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.158955</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.227971</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.122030</td>\n",
       "      <td>-0.247205</td>\n",
       "      <td>-0.351799</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>0.323136</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>1.900210</td>\n",
       "      <td>0.781971</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.502984</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.262476</td>\n",
       "      <td>1.709092</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.053033</td>\n",
       "      <td>0.199247</td>\n",
       "      <td>-0.160469</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>2.182437</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.563927</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.042820</td>\n",
       "      <td>-0.221546</td>\n",
       "      <td>-0.429321</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.178673</td>\n",
       "      <td>-0.110827</td>\n",
       "      <td>-0.358383</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.665898</td>\n",
       "      <td>-0.167647</td>\n",
       "      <td>0.091447</td>\n",
       "      <td>-0.212690</td>\n",
       "      <td>1.650956</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014070</td>\n",
       "      <td>-0.140684</td>\n",
       "      <td>-0.008922</td>\n",
       "      <td>-0.023383</td>\n",
       "      <td>-0.121253</td>\n",
       "      <td>0.034386</td>\n",
       "      <td>-0.082661</td>\n",
       "      <td>0.070956</td>\n",
       "      <td>3.258733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>3.907825</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>1.501060</td>\n",
       "      <td>0.980185</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.662016</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.010544</td>\n",
       "      <td>-0.206152</td>\n",
       "      <td>-0.411177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>1.260747</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.022308</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.487908</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.127987</td>\n",
       "      <td>-0.242073</td>\n",
       "      <td>-0.353448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.150725</td>\n",
       "      <td>0.488862</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>0.008652</td>\n",
       "      <td>-0.154835</td>\n",
       "      <td>-0.332006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.178673</td>\n",
       "      <td>-0.110827</td>\n",
       "      <td>-0.358383</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.665898</td>\n",
       "      <td>-0.167647</td>\n",
       "      <td>0.091447</td>\n",
       "      <td>-0.212690</td>\n",
       "      <td>1.650956</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014070</td>\n",
       "      <td>-0.140684</td>\n",
       "      <td>-0.008922</td>\n",
       "      <td>-0.023383</td>\n",
       "      <td>-0.121253</td>\n",
       "      <td>0.034386</td>\n",
       "      <td>-0.082787</td>\n",
       "      <td>0.070956</td>\n",
       "      <td>3.257083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.150725</td>\n",
       "      <td>0.488862</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>0.008652</td>\n",
       "      <td>-0.154835</td>\n",
       "      <td>-0.332006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>3.949404</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.121299</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.081338</td>\n",
       "      <td>-0.206152</td>\n",
       "      <td>-0.389735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.787825</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.101681</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.037556</td>\n",
       "      <td>-0.211283</td>\n",
       "      <td>-0.407878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.502315</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>1.368916</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>1.183643</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.218336</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.078942</td>\n",
       "      <td>0.070956</td>\n",
       "      <td>-0.254484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.355778</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.122276</td>\n",
       "      <td>0.489780</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.210969</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.229371</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.124047</td>\n",
       "      <td>-0.242073</td>\n",
       "      <td>-0.305616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.425300</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.132116</td>\n",
       "      <td>-0.262599</td>\n",
       "      <td>-0.445814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.199680</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.196632</td>\n",
       "      <td>0.307162</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022899</td>\n",
       "      <td>-0.392232</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.208527</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.116703</td>\n",
       "      <td>-0.211283</td>\n",
       "      <td>-0.241289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>0.607376</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.018536</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>0.078467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.118488</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.125182</td>\n",
       "      <td>-0.236941</td>\n",
       "      <td>-0.366643</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>1.769215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.273631</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.109422</td>\n",
       "      <td>-0.211283</td>\n",
       "      <td>-0.396333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.029706</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.116734</td>\n",
       "      <td>-0.242073</td>\n",
       "      <td>-0.361695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>1.823776</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.125812</td>\n",
       "      <td>-0.252336</td>\n",
       "      <td>-0.427671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.236752</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>1.170428</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.120391</td>\n",
       "      <td>-0.242073</td>\n",
       "      <td>-0.351799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>0.541872</td>\n",
       "      <td>-0.126325</td>\n",
       "      <td>-0.358383</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>2.334924</td>\n",
       "      <td>-0.277555</td>\n",
       "      <td>0.173923</td>\n",
       "      <td>-0.262476</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>0.367490</td>\n",
       "      <td>-0.044932</td>\n",
       "      <td>-0.047071</td>\n",
       "      <td>2.483516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4580</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.114118</td>\n",
       "      <td>-0.242073</td>\n",
       "      <td>-0.449113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4581</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>1.512446</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.177676</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.218336</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.113645</td>\n",
       "      <td>-0.247205</td>\n",
       "      <td>-0.391384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.123732</td>\n",
       "      <td>-0.252336</td>\n",
       "      <td>-0.435918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4583</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>1.883289</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>3.556555</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.111124</td>\n",
       "      <td>-0.201020</td>\n",
       "      <td>-0.351799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.335941</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>0.453257</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.210969</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.279642</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.116356</td>\n",
       "      <td>-0.231810</td>\n",
       "      <td>-0.264381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4585</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>1.797717</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.120296</td>\n",
       "      <td>-0.247205</td>\n",
       "      <td>-0.449113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.071050</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.099992</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>0.027398</td>\n",
       "      <td>-0.107121</td>\n",
       "      <td>-0.159967</td>\n",
       "      <td>-0.180261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.051902</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.123543</td>\n",
       "      <td>-0.247205</td>\n",
       "      <td>-0.421074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>5.454094</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.128617</td>\n",
       "      <td>-0.257468</td>\n",
       "      <td>-0.450763</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.338732</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.132116</td>\n",
       "      <td>-0.262599</td>\n",
       "      <td>-0.430970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.170052</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>0.111256</td>\n",
       "      <td>-0.085845</td>\n",
       "      <td>-0.211283</td>\n",
       "      <td>-0.336954</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4591</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.132116</td>\n",
       "      <td>-0.262599</td>\n",
       "      <td>-0.454061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>1.922965</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>3.253491</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.103691</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.123133</td>\n",
       "      <td>-0.247205</td>\n",
       "      <td>-0.422723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4593</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.959963</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.132116</td>\n",
       "      <td>-0.262599</td>\n",
       "      <td>-0.457360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>1.816213</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.109201</td>\n",
       "      <td>-0.242073</td>\n",
       "      <td>-0.435918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>1.803938</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.132116</td>\n",
       "      <td>-0.262599</td>\n",
       "      <td>-0.427671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.672880</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.673183</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>0.781971</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.343917</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.127640</td>\n",
       "      <td>-0.252336</td>\n",
       "      <td>-0.322110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.102907</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.114623</td>\n",
       "      <td>-0.247205</td>\n",
       "      <td>-0.444165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.640128</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.038373</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260533</td>\n",
       "      <td>2.141746</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.119382</td>\n",
       "      <td>-0.236941</td>\n",
       "      <td>-0.272628</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>2.801763</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.303450</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.127483</td>\n",
       "      <td>-0.242073</td>\n",
       "      <td>-0.338604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.732697</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.176648</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.124236</td>\n",
       "      <td>-0.242073</td>\n",
       "      <td>-0.401281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0          -0.342434           0.330885       0.712859       -0.0469   \n",
       "1           0.345359           0.051909       0.435130       -0.0469   \n",
       "2          -0.145921          -0.165072       0.851723       -0.0469   \n",
       "3          -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4          -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "5          -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "6          -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "7          -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "8           0.148847          -0.165072       0.355778       -0.0469   \n",
       "9          -0.145921          -0.072080       0.970750       -0.0469   \n",
       "10         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "11         -0.342434          -0.165072      -0.060816       -0.0469   \n",
       "12         -0.342434           0.369632       0.117725       -0.0469   \n",
       "13         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "14         -0.342434          -0.165072       2.260207       -0.0469   \n",
       "15         -0.342434           0.160400       0.276427       -0.0469   \n",
       "16         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "17         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "18         -0.342434          -0.165072       0.534319       -0.0469   \n",
       "19         -0.342434           0.323136      -0.556761       -0.0469   \n",
       "20         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "21         -0.178673          -0.110827      -0.358383       -0.0469   \n",
       "22         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "23         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "24         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "25         -0.178673          -0.110827      -0.358383       -0.0469   \n",
       "26         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "27         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "28         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "29         -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "...              ...                ...            ...           ...   \n",
       "4571       -0.342434          -0.165072       0.355778       -0.0469   \n",
       "4572       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4573       -0.342434          -0.165072      -0.199680       -0.0469   \n",
       "4574        0.607376          -0.165072       0.018536       -0.0469   \n",
       "4575       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4576       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4577       -0.342434          -0.165072       1.823776       -0.0469   \n",
       "4578       -0.342434          -0.165072       0.236752       -0.0469   \n",
       "4579        0.541872          -0.126325      -0.358383       -0.0469   \n",
       "4580       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4581       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4582       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4583       -0.342434          -0.165072       1.883289       -0.0469   \n",
       "4584       -0.342434          -0.165072       0.335941       -0.0469   \n",
       "4585       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4586       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4587       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4588       -0.342434          -0.165072       5.454094       -0.0469   \n",
       "4589       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4590       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4591       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4592       -0.342434          -0.165072       1.922965       -0.0469   \n",
       "4593       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4594       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4595       -0.342434          -0.165072       1.803938       -0.0469   \n",
       "4596        0.672880          -0.165072       0.673183       -0.0469   \n",
       "4597       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4598        0.640128          -0.165072       0.038373       -0.0469   \n",
       "4599        2.801763          -0.165072      -0.556761       -0.0469   \n",
       "4600       -0.342434          -0.165072       0.732697       -0.0469   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0          0.011565       -0.350266         -0.291794           -0.262562   \n",
       "1         -0.256117        0.672399          0.244743           -0.088010   \n",
       "2          1.364846        0.343685          0.193644            0.036670   \n",
       "3          0.472573       -0.350266          0.500237            1.308402   \n",
       "4          0.472573       -0.350266          0.500237            1.308402   \n",
       "5          2.286862       -0.350266         -0.291794            4.350584   \n",
       "6          2.390960       -0.350266         -0.291794           -0.262562   \n",
       "7          2.331475       -0.350266         -0.291794            4.425392   \n",
       "8          0.442830       -0.350266          0.474688           -0.262562   \n",
       "9         -0.181761        0.818494          0.679083           -0.262562   \n",
       "10        -0.464314       -0.350266          2.160947           -0.262562   \n",
       "11         0.100792        0.562828          0.346941           -0.262562   \n",
       "12         0.041307       -0.350266         -0.291794           -0.262562   \n",
       "13         0.874096       -0.350266          2.007651           -0.262562   \n",
       "14         0.591542        0.928066         -0.291794            0.610196   \n",
       "15         1.424331       -0.350266          0.781280           -0.262562   \n",
       "16         0.933580       -0.350266         -0.291794           -0.262562   \n",
       "17        -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "18         1.186391       -0.350266          0.168095           -0.262562   \n",
       "19         1.900210        0.781971         -0.291794           -0.262562   \n",
       "20        -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "21         0.665898       -0.167647          0.091447           -0.212690   \n",
       "22         3.907825       -0.350266         -0.291794           -0.262562   \n",
       "23         1.260747       -0.350266         -0.291794           -0.262562   \n",
       "24        -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "25         0.665898       -0.167647          0.091447           -0.212690   \n",
       "26        -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "27        -0.464314       -0.350266          3.949404           -0.262562   \n",
       "28        -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "29         0.502315       -0.350266          1.368916           -0.262562   \n",
       "...             ...             ...               ...                 ...   \n",
       "4571      -0.122276        0.489780         -0.291794           -0.262562   \n",
       "4572      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4573      -0.196632        0.307162         -0.291794           -0.262562   \n",
       "4574      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4575      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4576      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4577      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4578      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4579      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4580      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4581      -0.464314        1.512446         -0.291794           -0.262562   \n",
       "4582      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4583      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4584      -0.464314        0.453257         -0.291794           -0.262562   \n",
       "4585      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4586       0.071050       -0.350266         -0.291794           -0.262562   \n",
       "4587      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4588      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4589       0.338732       -0.350266         -0.291794           -0.262562   \n",
       "4590      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4591      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4592       3.253491       -0.350266         -0.291794           -0.262562   \n",
       "4593      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4594      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4595      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4596      -0.464314        0.781971         -0.291794           -0.262562   \n",
       "4597      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4598      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "4599       0.011565       -0.350266         -0.291794           -0.262562   \n",
       "4600      -0.464314       -0.350266         -0.291794           -0.262562   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...   char_freq_;  char_freq_(  \\\n",
       "0           -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "1           -0.323302        1.086711  ...     -0.158453    -0.026007   \n",
       "2            1.974017        0.016422  ...     -0.117376     0.014684   \n",
       "3            0.789462        0.605857  ...     -0.158453    -0.007511   \n",
       "4            0.789462        0.605857  ...     -0.158453    -0.014910   \n",
       "5           -0.323302       -0.371364  ...     -0.158453     0.310623   \n",
       "6           -0.323302        0.621368  ...     -0.158453    -0.314548   \n",
       "7           -0.323302       -0.371364  ...     -0.158453     0.247736   \n",
       "8            2.979094        0.807505  ...     -0.158453     0.488187   \n",
       "9           -0.107929       -0.371364  ...      0.005855    -0.403329   \n",
       "10          -0.323302        2.606833  ...     -0.158453    -0.514307   \n",
       "11          -0.323302       -0.371364  ...     -0.068084    -0.351540   \n",
       "12          -0.323302       -0.371364  ...     -0.158453    -0.307149   \n",
       "13          -0.323302        1.024666  ...     -0.158453    -0.514307   \n",
       "14          -0.323302        0.729948  ...     -0.158453    -0.136984   \n",
       "15          -0.323302        1.598589  ...     -0.158453    -0.281255   \n",
       "16          -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "17          -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "18          -0.323302       -0.371364  ...     -0.158453     0.158955   \n",
       "19           0.789462       -0.371364  ...     -0.158453     0.502984   \n",
       "20          -0.323302       -0.371364  ...     -0.158453     2.182437   \n",
       "21           1.650956       -0.371364  ...      0.014070    -0.140684   \n",
       "22          -0.323302       -0.371364  ...      1.501060     0.980185   \n",
       "23          -0.323302       -0.371364  ...     -0.158453    -0.022308   \n",
       "24          -0.323302       -0.371364  ...     -0.158453     0.210744   \n",
       "25           1.650956       -0.371364  ...      0.014070    -0.140684   \n",
       "26          -0.323302       -0.371364  ...     -0.158453     0.210744   \n",
       "27          -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "28          -0.323302       -0.371364  ...     -0.158453     0.787825   \n",
       "29          -0.323302       -0.371364  ...     -0.158453     1.183643   \n",
       "...               ...             ...  ...           ...          ...   \n",
       "4571        -0.323302       -0.371364  ...     -0.158453    -0.210969   \n",
       "4572        -0.323302       -0.371364  ...     -0.158453     0.425300   \n",
       "4573        -0.323302       -0.371364  ...     -0.022899    -0.392232   \n",
       "4574        -0.323302        0.078467  ...     -0.158453    -0.118488   \n",
       "4575        -0.323302        1.769215  ...     -0.158453     0.273631   \n",
       "4576        -0.323302       -0.371364  ...     -0.158453    -0.029706   \n",
       "4577        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4578        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4579        -0.323302       -0.371364  ...      2.334924    -0.277555   \n",
       "4580        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4581        -0.323302       -0.371364  ...     -0.158453    -0.177676   \n",
       "4582        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4583        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4584        -0.323302       -0.371364  ...     -0.158453    -0.210969   \n",
       "4585        -0.323302       -0.371364  ...     -0.158453     1.797717   \n",
       "4586        -0.323302       -0.371364  ...     -0.158453    -0.099992   \n",
       "4587        -0.323302       -0.371364  ...     -0.158453    -0.051902   \n",
       "4588        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4589        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4590        -0.323302       -0.371364  ...     -0.158453     0.170052   \n",
       "4591        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4592        -0.323302       -0.371364  ...     -0.158453    -0.103691   \n",
       "4593        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4594        -0.323302       -0.371364  ...     -0.158453     1.816213   \n",
       "4595        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4596        -0.323302       -0.371364  ...     -0.158453     0.343917   \n",
       "4597        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "4598        -0.323302       -0.371364  ...      0.260533     2.141746   \n",
       "4599        -0.323302       -0.371364  ...     -0.158453    -0.303450   \n",
       "4600        -0.323302       -0.371364  ...     -0.158453    -0.514307   \n",
       "\n",
       "      char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0       -0.155198     0.624007    -0.308355    -0.103048   \n",
       "1       -0.155198     0.126203     0.423783     0.008763   \n",
       "2       -0.155198     0.008496     0.440053    -0.079754   \n",
       "3       -0.155198    -0.161934    -0.308355    -0.103048   \n",
       "4       -0.155198    -0.164387    -0.308355    -0.103048   \n",
       "5       -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "6       -0.155198    -0.128829    -0.088714    -0.103048   \n",
       "7       -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "8       -0.155198    -0.107985     0.517334    -0.051802   \n",
       "9       -0.155198    -0.030740     0.021107    -0.103048   \n",
       "10      -0.155198     0.236554    -0.308355    -0.103048   \n",
       "11      -0.155198     0.483003    -0.308355    -0.103048   \n",
       "12      -0.155198     0.633816    -0.308355    -0.103048   \n",
       "13      -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "14      -0.155198     0.107811    -0.308355    -0.103048   \n",
       "15      -0.155198     0.371427    -0.052107    -0.103048   \n",
       "16      -0.155198     0.194866    -0.308355    -0.103048   \n",
       "17      -0.155198     2.091668     1.196595    -0.103048   \n",
       "18      -0.155198     0.227971    -0.308355    -0.103048   \n",
       "19      -0.155198    -0.262476     1.709092    -0.103048   \n",
       "20      -0.155198     0.563927    -0.308355    -0.103048   \n",
       "21      -0.008922    -0.023383    -0.121253     0.034386   \n",
       "22      -0.155198     0.662016    -0.308355    -0.103048   \n",
       "23      -0.155198     0.487908    -0.308355    -0.103048   \n",
       "24      -0.155198     0.150725     0.488862    -0.103048   \n",
       "25      -0.008922    -0.023383    -0.121253     0.034386   \n",
       "26      -0.155198     0.150725     0.488862    -0.103048   \n",
       "27      -0.155198     0.121299    -0.308355    -0.103048   \n",
       "28      -0.155198     0.101681    -0.308355    -0.103048   \n",
       "29      -0.155198    -0.218336    -0.308355    -0.103048   \n",
       "...           ...          ...          ...          ...   \n",
       "4571    -0.155198    -0.229371    -0.308355    -0.103048   \n",
       "4572    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4573    -0.155198    -0.208527    -0.308355    -0.103048   \n",
       "4574    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4575    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4576    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4577    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4578     1.170428    -0.329912    -0.308355    -0.103048   \n",
       "4579     0.173923    -0.262476    -0.308355     0.367490   \n",
       "4580    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4581    -0.155198    -0.218336    -0.308355    -0.103048   \n",
       "4582    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4583     3.556555    -0.329912    -0.308355    -0.103048   \n",
       "4584    -0.155198    -0.279642    -0.308355    -0.103048   \n",
       "4585    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4586    -0.155198    -0.329912    -0.308355     0.027398   \n",
       "4587    -0.155198    -0.329912     0.200074    -0.103048   \n",
       "4588    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4589    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4590    -0.155198    -0.329912    -0.308355     0.111256   \n",
       "4591    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4592    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4593    -0.155198     0.959963    -0.308355    -0.103048   \n",
       "4594    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4595    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4596    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4597    -0.155198     0.102907    -0.308355    -0.103048   \n",
       "4598    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4599    -0.155198    -0.329912    -0.308355    -0.103048   \n",
       "4600    -0.155198    -0.176648    -0.308355    -0.103048   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "0                      -0.045247                    0.045298   \n",
       "1                      -0.002443                    0.250563   \n",
       "2                       0.145921                    2.221106   \n",
       "3                      -0.052150                   -0.062466   \n",
       "4                      -0.052150                   -0.062466   \n",
       "5                      -0.069076                   -0.190757   \n",
       "6                      -0.110966                   -0.247205   \n",
       "7                      -0.086412                   -0.211283   \n",
       "8                       0.143494                    2.015841   \n",
       "9                      -0.109138                   -0.047071   \n",
       "10                     -0.122282                   -0.236941   \n",
       "11                     -0.124457                   -0.211283   \n",
       "12                     -0.046130                    0.045298   \n",
       "13                     -0.097980                   -0.231810   \n",
       "14                     -0.101510                   -0.144572   \n",
       "15                      0.014735                    0.014508   \n",
       "16                     -0.017005                   -0.108651   \n",
       "17                      0.954091                    0.219773   \n",
       "18                     -0.122030                   -0.247205   \n",
       "19                     -0.053033                    0.199247   \n",
       "20                     -0.042820                   -0.221546   \n",
       "21                     -0.082661                    0.070956   \n",
       "22                     -0.010544                   -0.206152   \n",
       "23                     -0.127987                   -0.242073   \n",
       "24                      0.008652                   -0.154835   \n",
       "25                     -0.082787                    0.070956   \n",
       "26                      0.008652                   -0.154835   \n",
       "27                     -0.081338                   -0.206152   \n",
       "28                     -0.037556                   -0.211283   \n",
       "29                     -0.078942                    0.070956   \n",
       "...                          ...                         ...   \n",
       "4571                   -0.124047                   -0.242073   \n",
       "4572                   -0.132116                   -0.262599   \n",
       "4573                   -0.116703                   -0.211283   \n",
       "4574                   -0.125182                   -0.236941   \n",
       "4575                   -0.109422                   -0.211283   \n",
       "4576                   -0.116734                   -0.242073   \n",
       "4577                   -0.125812                   -0.252336   \n",
       "4578                   -0.120391                   -0.242073   \n",
       "4579                   -0.044932                   -0.047071   \n",
       "4580                   -0.114118                   -0.242073   \n",
       "4581                   -0.113645                   -0.247205   \n",
       "4582                   -0.123732                   -0.252336   \n",
       "4583                   -0.111124                   -0.201020   \n",
       "4584                   -0.116356                   -0.231810   \n",
       "4585                   -0.120296                   -0.247205   \n",
       "4586                   -0.107121                   -0.159967   \n",
       "4587                   -0.123543                   -0.247205   \n",
       "4588                   -0.128617                   -0.257468   \n",
       "4589                   -0.132116                   -0.262599   \n",
       "4590                   -0.085845                   -0.211283   \n",
       "4591                   -0.132116                   -0.262599   \n",
       "4592                   -0.123133                   -0.247205   \n",
       "4593                   -0.132116                   -0.262599   \n",
       "4594                   -0.109201                   -0.242073   \n",
       "4595                   -0.132116                   -0.262599   \n",
       "4596                   -0.127640                   -0.252336   \n",
       "4597                   -0.114623                   -0.247205   \n",
       "4598                   -0.119382                   -0.236941   \n",
       "4599                   -0.127483                   -0.242073   \n",
       "4600                   -0.124236                   -0.242073   \n",
       "\n",
       "      capital_run_length_total  spam  \n",
       "0                    -0.008724     1  \n",
       "1                     1.228324     1  \n",
       "2                     3.258733     1  \n",
       "3                    -0.152222     1  \n",
       "4                    -0.152222     1  \n",
       "5                    -0.378189     1  \n",
       "6                    -0.282524     1  \n",
       "7                    -0.386436     1  \n",
       "8                     1.606036     1  \n",
       "9                     0.768142     1  \n",
       "10                   -0.432619     1  \n",
       "11                   -0.163768     1  \n",
       "12                   -0.036764     1  \n",
       "13                   -0.426022     1  \n",
       "14                   -0.129130     1  \n",
       "15                   -0.056557     1  \n",
       "16                   -0.290771     1  \n",
       "17                    0.293116     1  \n",
       "18                   -0.351799     1  \n",
       "19                   -0.160469     1  \n",
       "20                   -0.429321     1  \n",
       "21                    3.258733     1  \n",
       "22                   -0.411177     1  \n",
       "23                   -0.353448     1  \n",
       "24                   -0.332006     1  \n",
       "25                    3.257083     1  \n",
       "26                   -0.332006     1  \n",
       "27                   -0.389735     1  \n",
       "28                   -0.407878     1  \n",
       "29                   -0.254484     1  \n",
       "...                        ...   ...  \n",
       "4571                 -0.305616     0  \n",
       "4572                 -0.445814     0  \n",
       "4573                 -0.241289     0  \n",
       "4574                 -0.366643     0  \n",
       "4575                 -0.396333     0  \n",
       "4576                 -0.361695     0  \n",
       "4577                 -0.427671     0  \n",
       "4578                 -0.351799     0  \n",
       "4579                  2.483516     0  \n",
       "4580                 -0.449113     0  \n",
       "4581                 -0.391384     0  \n",
       "4582                 -0.435918     0  \n",
       "4583                 -0.351799     0  \n",
       "4584                 -0.264381     0  \n",
       "4585                 -0.449113     0  \n",
       "4586                 -0.180261     0  \n",
       "4587                 -0.421074     0  \n",
       "4588                 -0.450763     0  \n",
       "4589                 -0.430970     0  \n",
       "4590                 -0.336954     0  \n",
       "4591                 -0.454061     0  \n",
       "4592                 -0.422723     0  \n",
       "4593                 -0.457360     0  \n",
       "4594                 -0.435918     0  \n",
       "4595                 -0.427671     0  \n",
       "4596                 -0.322110     0  \n",
       "4597                 -0.444165     0  \n",
       "4598                 -0.272628     0  \n",
       "4599                 -0.338604     0  \n",
       "4600                 -0.401281     0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "resc_data = spam_data.copy()\n",
    "\n",
    "for lb in [x for x in resc_data.columns.values if x != 'spam']:\n",
    "    resc_data[lb] = scale(resc_data[lb])\n",
    "resc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = resc_data.iloc[train_index]\n",
    "test = resc_data.iloc[test_index]\n",
    "\n",
    "train, train_res = train.drop('spam', 1), list(train['spam'])\n",
    "test, test_res = test.drop('spam', 1), list(test['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.896958410925\n",
      "Precision = 0.910034602076\n",
      "Recall = 0.821875\n",
      "F1 = 0.863711001642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "\n",
    "X, y = train, train_res\n",
    "neigh.fit(X, y) \n",
    "\n",
    "knnr = {}\n",
    "knnr['pred'] = neigh.predict(test)\n",
    "knnr['pred_prob'] = [x[1] for x in neigh.predict_proba(test)]\n",
    "get_confus_matrix(knnr, test_res)\n",
    "print_confus_matrix(knnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.791433891993\n",
      "Precision = 0.781481481481\n",
      "Recall = 0.659375\n",
      "F1 = 0.715254237288\n"
     ]
    }
   ],
   "source": [
    "X, y = train, train_res\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=7, random_state=random_state).fit(X, y)\n",
    "\n",
    "dtsr = {}\n",
    "dtsr['pred'] = clf.predict(test)\n",
    "dtsr['pred_prob'] = [x[1] for x in clf.predict_proba(test)]\n",
    "get_confus_matrix(knn, test_res)\n",
    "print_confus_matrix(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare all models by all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Constant</th>\n",
       "      <td>0.397269</td>\n",
       "      <td>0.397269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.568636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision tree</th>\n",
       "      <td>0.914960</td>\n",
       "      <td>0.915702</td>\n",
       "      <td>0.865625</td>\n",
       "      <td>0.889960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.791434</td>\n",
       "      <td>0.781481</td>\n",
       "      <td>0.659375</td>\n",
       "      <td>0.715254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN rescaled</th>\n",
       "      <td>0.896958</td>\n",
       "      <td>0.910035</td>\n",
       "      <td>0.821875</td>\n",
       "      <td>0.863711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Precision    Recall        F1\n",
       "Constant       0.397269   0.397269  1.000000  0.568636\n",
       "Decision tree  0.914960   0.915702  0.865625  0.889960\n",
       "KNN            0.791434   0.781481  0.659375  0.715254\n",
       "KNN rescaled   0.896958   0.910035  0.821875  0.863711"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = pd.DataFrame({})\n",
    "stats = stats.reindex(index=['Constant', 'Decision tree', 'KNN', 'KNN rescaled'])\n",
    "stats['Accuracy'] = [const1['acc'], dts['acc'], knn['acc'], knnr['acc']]\n",
    "stats['Precision'] = [const1['prec'], dts['prec'], knn['prec'], knnr['prec']]\n",
    "stats['Recall'] = [const1['rec'], dts['rec'], knn['rec'], knnr['rec']]\n",
    "stats['F1'] = [const1['f1'], dts['f1'], knn['f1'], knnr['f1']]\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
